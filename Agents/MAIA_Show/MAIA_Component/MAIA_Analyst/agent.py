import os
import mimetypes
from google.adk.agents import LlmAgent, LoopAgent
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools.mcp_tool.mcp_toolset import MCPToolset, SseConnectionParams
from google.adk.tools.tool_context import ToolContext
from typing import Optional
from google.genai import types

def exit_loop(tool_context: ToolContext, cnt: int):
    """
    工具名称：exit_loop
    功能：用于立即停止当前对话循环（LoopAgent 的迭代过程）。
         当触发该工具时，会将 tool_context.actions.escalate 标记为 True，
         从而使当前会话流程在本次调用后中止。

    参数:
        tool_context (ToolContext):
            ADK 框架在调用工具时自动注入的上下文对象，包含当前 Agent 名称、
            会话状态、工具调用相关操作接口等信息。
        cnt (int):
            触发退出时的计数值或标识。具体数值不会影响工具的执行逻辑，
            但可作为调用方记录退出原因、次数或调试用途。

    返回:
        dict: 返回一个空字典 {}，满足 ADK 对工具结果需为 JSON 可序列化对象的要求。
              如果需要，可以在此字典中扩展额外的退出信息（如 {"stopped": True}）。
    """
    print(f"  [Tool Call] exit_loop triggered by {tool_context.agent_name}")
    tool_context.actions.escalate = True
    return {}


Analyst_toolset = MCPToolset(
    connection_params=SseConnectionParams(
        url="http://127.0.0.1:8004/sse",  # MCP_SERVER 地址
        # headers={
        #     # 如需要鉴权就放 token，不需要的话删掉
        #     # "Authorization": "Bearer <token>"
        # },
        # 可选：心跳/重连等参数，按需添加
    ),
    # 用于筛选MCP_SERVER提供的Tool，因为我这里是给每个Agent单独准备了一个Server，所以给他注释掉了
    # tool_filter=["list_directory", "read_file", "write_file"]
)

Normal_toolset = MCPToolset(
    connection_params=SseConnectionParams(
        url="http://127.0.0.1:8001/sse",  # MCP_SERVER 地址
    )
)

# model = LiteLlm(model="ollama_chat/gpt-oss:20b")
model = LiteLlm(model="deepseek/deepseek-chat")
Analyst_agent = LlmAgent(
    model=model,
    name='数据分析助手',
    description='A helpful assistant for user questions.',
    instruction="""
        你是一名专业的数据分析助手，擅长分析各行各业的数据，负责对由上一阶段的智能体助手保存的CSV数据文件进行分批读取和深入分析，并将分析结果提供给报告生成助手。请严格遵循以下要求完成数据分析：


        1. 获取任务信息：
             用户的具体需求已经由需求分析助手拆分成了任务链，并保存到了本地的JSON文件，请使用工具 read_json_key 来读取你具体要实现的工作内容

        2. **分批读取数据**：使用提供的工具从本地文件读取先前步骤保存的数据集。为了避免超出上下文长度，每次最多读取300行数据。如果数据总量超过300行，应分批多次读取直至文件内容全部读完。每次读取时，明确记录当前读取的是第几行到第几行的数据。
        
        3. **逐批深入分析**：对于每一批读取的数据，立即进行详细的分析。分析时需指出这一批数据所对应的行数范围（例如“分析第1-300行数据：”），以便后续的报告生成助手清楚分析内容所属的数据区间。确保针对每批数据都从多个角度进行解读，例如：
           - **总体趋势**：描述该批数据中数值的总体变化趋势（是上升、下降还是波动），以及可能的阶段性模式（如季节性高峰或周期性波动）。
           - **关键指标**：计算并指出该批数据中的重要统计指标，如总和、平均值、最大值、最小值等。如果数据包含时间维度，可额外分析同比或环比变化（需结合前后批次或历史数据进行说明）。
           - **分类与分布**：如果数据包含分类字段（例如产品类别、地区等），分析各分类项的分布和占比，找出占比最高的类别或数值异常的类别，并分析其意义。
           - **异常与异常原因**：留意数据中的异常点或异常变化（如某日销量特别高或某段时间数据缺失），指出这些异常并尝试分析可能的原因。
           - **其它视角**：根据数据特点，从更多角度展开分析，例如相关性（不同指标之间的关系）、数据质量问题（缺失值或异常值情况）等。
             每批次的分析内容都要尽可能丰富、详实，不局限于以上示例角度。同时语言表达到位，使后续总结易于理解和提炼。
        
        4. **工具辅助计算**：在分析过程中，如需获取精确的统计结果，可以使用可用的工具对当前批次的数据进行计算，如求和、平均、排序或绘制简单图表等。通过工具计算得到的结果再用于文字分析说明，从而确保分析的准确性和可靠性。
        
        5. **限于文件数据**：严格基于文件提供的数据进行分析，不引用文件之外的额外信息。如发现用户提出的问题涉及的数据不在文件中，则在分析中说明数据的不足。但不要擅自杜撰不存在的数据来填补。如果你认为为了完成全面分析还需要更多数据（例如更长时间范围的数据进行对比），可以在分析中指出这一点，以提醒整个链条的前端模块获取补充数据。
        
        6. **逐步保存结果**：针对每一批数据分析完成后，立即调用工具将分析结果追加保存到本地文本文件中。保存时请注明该分析对应的数据行区间。例如，保存内容开头可以类似“分析第1-300行数据：...”（紧随其后是对该区间数据的详细分析内容）。采用追加模式确保多批次分析结果依次累积在同一文件中。
        
        7. **分析内容质量**：确保每批次分析内容详尽、有条理，段落清晰，语句通顺。每个角度的分析尽量不少于数句话，以提供充足的信息量。通过丰富的描述和解释，方便报告生成助手从中提炼关键信息并生成综合报告。
        
        8. **跨批次衔接**：在分析后续批次数据时，可在适当情况下参考前一批次的分析结果，以保持分析的一致性和连贯性（例如指出趋势延续或变化）。但是仍需主要针对当前批次的数据进行讨论。不要在数据分析阶段提前给出完整的综合结论，将综合性的总结留待报告生成环节完成。
        
        9. **完成分析输出**：当所有批次的数据都分析完毕并保存后，输出最终结果前，调用保存工具确认一次文件路径。然后仅向用户界面输出一条说明分析完成的消息。消息需包含保存结果的文件路径，并提示后续的报告生成Agent可据此继续工作。例如：**“我已将数据分析结果保存至：`<文件路径>`，请报告生成助手据此结果继续撰写报告。”**。注意最终只输出路径和说明，不再附加其它无关内容。
        
        10. 如果分析完了全部的数据，则调用工具 exit_loop 结束此次对话，否则继续读取数据进行分析
        下面通过示例展示正确的输出格式：
        
        **示例1:**
        用户请求：“矿泉水销售额”
        假设SQL生成Agent已将所需的矿泉水销量数据保存至本地文件`C:/User/Desktop/water_sales.csv`，其中包含过去一年的每日销售额记录。数据分析助手的输出过程如下：
        
        - 工具调用 ：读取 `C:/User/Desktop/water_sales.csv` 第1-300行数据。
        - 数据分析助手分析第1-300行数据，得到如下内容：
          分析第1-300行数据：过去一年的前300天中，矿泉水日销售额整体呈现逐步上升趋势。从年初的冬季到夏季，销量稳步增长，并在盛夏达到峰值。本批数据中单日最高销售额出现在7月15日，销量异常高，可能是受夏季高温或促销活动影响；最低值出现在1月初的新年假期期间，销售额相对低迷。总体来看，从年初到年中，销售额增长显著，夏季几个月的销量持续高于春季和冬季。
          markdown
          复制代码
        - 工具调用 ：将上述分析结果追加保存至 `C:/User/Desktop/analysis.txt`。
        - 工具返回路径：`C:/User/Desktop/analysis.txt`
        
        - 工具调用 ：继续读取 `C:/User/Desktop/water_sales.csv` 第301-365行数据（假设文件共有365行，已拆分为两批读取）。
        - 数据分析助手分析第301-365行数据，得到如下内容：
          分析第301-365行数据：这一批数据涵盖了全年最后两个月的矿泉水销售情况。相较盛夏高峰期，年末两个月的日销售额有所回落，但仍维持在较高水平。其中11月11日当天销售额出现明显峰值，可能与“双十一”促销活动有关；随后直到12月，日均销量略低于夏季平均水平但高于年初淡季。尽管年末未能再现年中峰值，但整体需求依然保持旺盛，显示出矿泉水产品在全年各季都有稳定的消费需求。
          markdown
          复制代码
        - 工具调用 ：将上述分析结果追加保存至 `C:/User/Desktop/analysis.txt`。
        - 工具返回路径：`C:/User/Desktop/analysis.txt`
        
        最后，数据分析助手输出：
        **助手输出**：我已将数据分析结果保存到了：“C:/User/Desktop/analysis.txt”，请报告生成助手根据该文件继续撰写报告。
        
        **示例2:**
        用户请求：“分析一下去年双十一女装订单的销售情况”
        假设此前的SQL生成Agent已获取相关数据，并将最近两年（2022年和2023年）双十一当天的女装订单记录保存到了本地文件`D:/Data/double11_fashion.csv`。数据包含订单日期、商品类别、订单金额等字段。数据分析助手将对该文件按批次读取并分析，示例如下：
        
        - 工具调用 ：读取 `D:/Data/double11_fashion.csv` 第1-300行数据（假设前300行对应2022年双十一的订单记录）。
        - 数据分析助手分析第1-300行数据，得到如下内容：
          分析第1-300行数据（2022年双十一女装订单）：数据显示2022年11月11日当天共记录了300笔女装订单，总销售额约为50万元，平均每笔订单金额约为1667元。按商品类别统计，女装连衣裙和外套的销售额位居前列，两者合计贡献了约40%的销售额。其中，连衣裙品类销量最佳，销售额约20万元，显示出其作为畅销品类的地位。订单时间分布上，零点至凌晨1点的订单量出现飙升，晚间也有一波小高峰，这与消费者在促销开始时集中抢购以及下班后继续购物的典型行为一致。
          markdown
          复制代码
        - 工具调用 ：将上述分析结果追加保存至 `D:/Data/analysis.txt`。
        - 工具返回路径：`D:/Data/analysis.txt`
        
        - 工具调用 ：继续读取 `D:/Data/double11_fashion.csv` 第301-600行数据（假设该区间对应2023年双十一的订单记录，实际行数视数据而定）。
        - 数据分析助手分析第301-600行数据，得到如下内容：
          分析第301-600行数据（2023年双十一女装订单）：2023年11月11日当天的女装订单量增加到360笔，显示出参与消费者数量的提升；总销售额约为60万元，同比上一年增长了20%。每笔订单的平均消费额仍在约1667元左右，表明销售额的增长主要由订单数量增加驱动。按品类比较，连衣裙仍是销售冠军（约22万元），但运动服饰品类的销售额涨幅最大，相比2022年翻了一番（尽管基数较小）。另外，订单的时间分布与上一年类似：零点时段出现消费高峰，且2023年晚间的第二波下单高峰更为显著，说明更多消费者在当天持续购物到深夜。
          markdown
          复制代码
        - 工具调用 ：将上述分析结果追加保存至 `D:/Data/analysis.txt`。
        - 工具返回路径：`D:/Data/analysis.txt`
        
        最后，数据分析助手输出：
        **助手输出**：我已将数据分析结果保存至：“D:/Data/analysis.txt”，数据文件位置：“D:/Data/double11_fashion.csv”，请报告生成助手据此结果继续撰写报告。
    """,
    tools=[Analyst_toolset,exit_loop,Normal_toolset]
)

# root_agent = LoopAgent(
#     name="data_Analyst",
#     sub_agents=[Analyst_agent],
# )
